import pandas as pd
from sqlalchemy import text
import numpy as np
import sys
# NEW: Import os and dotenv to handle environment variables
import os
from dotenv import load_dotenv
# NEW: Import the Flask app and db object to create tables
from app import app, db, CustomerRecord, User, generate_password_hash

load_dotenv() # ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .env

# NEW: Function to create tables based on models in app.py
# ==============================================================================
# 1. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# ==============================================================================
def create_tables(clean_install=False):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    - ‡∏ñ‡πâ‡∏≤ clean_install=True: ‡∏à‡∏∞‡∏•‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏¥‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢!)
    - ‡∏ñ‡πâ‡∏≤ clean_install=False: ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢)
    """
    print("\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    try:
        with app.app_context():
            if clean_install:
                print("  - ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á --clean: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...")
                db.drop_all()
                print("  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î...")
                db.create_all()
            else:
                print("  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ (‡∏´‡∏≤‡∏Å‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)...")
                db.create_all() # ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        print("‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á: {e}")
        exit() # Exit if tables can't be created, as migration will fail anyway.

# ==============================================================================
# 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ)
# ==============================================================================
def process_dataframe_and_import(df, table_name, column_map, source_name):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á DataFrame ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏"""
    try:
        print(f"  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å '{source_name}' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á '{table_name}'...")
        # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        df.rename(columns=column_map, inplace=True)

        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô column_map ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏Å‡∏¥‡∏ô
        # ‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyError ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô map ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô df
        valid_columns = [col for col in column_map.values() if col in df.columns]
        df = df[valid_columns]

        if table_name == 'customer_records' and 'customer_id' in df.columns:
            # --- REVISED: Generate numeric IDs for missing values, without "PID-" prefix ---
            with app.app_context():
                # This query finds the highest numeric ID by casting the column to an integer.
                last_id_scalar = db.session.query(db.func.max(db.func.cast(CustomerRecord.customer_id, db.Integer))).scalar()
                
                id_counter = (last_id_scalar or 1000) + 1

                def generate_id(value):
                    nonlocal id_counter
                    # Check if the value is missing (NaN, None, or empty string)
                    if pd.isna(value) or str(value).strip() == '':
                        new_id = f"{id_counter}"
                        id_counter += 1
                        return new_id
                    # If the value exists, keep it as is.
                    return str(value).strip()

            df['customer_id'] = df['customer_id'].apply(generate_id)

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        for col in df.columns:
            if 'amount' in col or 'balance' in col or 'limit' in col or 'interest' in col or 'fee' in col:
                df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '', regex=False), errors='coerce').fillna(0)
            if 'date' in col or 'timestamp' in col:
                df[col] = pd.to_datetime(df[col], errors='coerce')
            elif 'time' in col: # NEW: Handle time columns
                df[col] = pd.to_datetime(df[col], errors='coerce', format='%H:%M:%S').dt.time

        # NEW: Hash passwords before inserting into the database
        if table_name == 'users' and 'password' in df.columns:
            print("  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏Æ‡∏ä‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô...")
            df['password'] = df['password'].apply(lambda pwd: generate_password_hash(str(pwd)) if pd.notna(pwd) else None)

            # --- NEW LOGIC TO PREVENT DUPLICATE USERS ---
            with app.app_context():
                # Get a list of all existing user IDs from the database
                print("  - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
                existing_user_ids = [user.user_id for user in db.session.query(User.user_id).all()]
                
                # Filter the DataFrame to only include users that are NOT already in the database
                original_count = len(df)
                df_new_users = df[~df['id'].isin(existing_user_ids)]
                new_count = len(df_new_users)

                if new_count == 0:
                    print("  - ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö")
                    return # Exit the function early if no new users
                
                print(f"  - ‡∏û‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {original_count} ‡∏Ñ‡∏ô‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå, ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡∏°‡πà {new_count} ‡∏Ñ‡∏ô")
                df = df_new_users # Replace the original df with the filtered one
            # --- END NEW LOGIC ---

        df.replace({np.nan: None, 'NaT': None}, inplace=True)

        # REVISED: ‡πÉ‡∏ä‡πâ engine ‡∏à‡∏≤‡∏Å app context ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á connection ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
        with app.app_context():
            df.to_sql(table_name, con=db.engine, if_exists='append', index=False)
        print(f"  ‚úÖ ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏π‡πà‡∏ï‡∏≤‡∏£‡∏≤‡∏á '{table_name}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    except Exception as e:
        print(f"  ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• '{source_name}' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á '{table_name}': {e}")


# ==============================================================================
# * 3. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤
# ==============================================================================

# --- 1. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö customer_records ---
customer_records_map = {
    'Timestamp': 'timestamp', 'Customer ID': 'customer_id', '‡∏ä‡∏∑‡πà‡∏≠': 'first_name', '‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•': 'last_name',
    '‡πÄ‡∏•‡∏Ç‡∏ö‡∏±‡∏ï‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô': 'id_card_number', '‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠': 'mobile_phone', '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å': 'main_customer_group',
    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏¢‡πà‡∏≠‡∏¢': 'sub_profession_group', '‡∏£‡∏∞‡∏ö‡∏∏‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏¢‡πà‡∏≠‡∏¢‡∏≠‡∏∑‡πà‡∏ô‡πÜ': 'other_sub_profession', '‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô': 'is_registered',
    '‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£': 'business_name', '‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà': 'province', '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô': 'registered_address',
    '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞': 'status', '‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£': 'desired_credit_limit', '‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥': 'approved_credit_limit',
    '‡πÄ‡∏Ñ‡∏¢‡∏Ç‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á': 'applied_before', '‡πÄ‡∏ä‡πá‡∏Ñ': 'check_status', '‡∏Ç‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ó‡∏≤‡∏á‡πÑ‡∏´‡∏ô': 'application_channel',
    '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô': 'assigned_company', '‡∏´‡∏±‡∏Å‡∏î‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏ó‡πâ‡∏≤‡∏¢': 'upfront_interest_deduction', '‡∏Ñ‡πà‡∏≤‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£': 'processing_fee', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤': 'application_date',
    '‡∏•‡∏¥‡∏á‡∏Ñ‡πå‡πÇ‡∏•‡πÄ‡∏Ñ‡∏ä‡∏±‡πà‡∏ô‡∏ö‡πâ‡∏≤‡∏ô': 'home_location_link', '‡∏•‡∏¥‡∏á‡∏Ñ‡πå‡πÇ‡∏•‡πÄ‡∏Ñ‡∏ä‡∏±‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô': 'work_location_link', '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏': 'remarks',
    'Image URLs': 'image_urls', 'Logged In User': 'logged_in_user', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏î‡∏ï‡∏£‡∏ß‡∏à': 'inspection_date',
    '‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏±‡∏î‡∏ï‡∏£‡∏ß‡∏à': 'inspection_time', '‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ï‡∏£‡∏ß‡∏à': 'inspector'
}

# --- 2. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö approove ---
approvals_map = {
    '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞': 'status', 'Customer ID': 'customer_id', '‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•': 'full_name', '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå': 'phone_number',
    '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥': 'approval_date', '‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥': 'approved_amount', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô': 'assigned_company',
    '‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô': 'registrar', '‡∏£‡∏π‡∏õ‡∏ñ‡πà‡∏≤‡∏¢‡∏™‡∏±‡∏ç‡∏ç‡∏≤': 'contract_image_urls'
}

# --- 3. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö bad_debt_records ---
bad_debt_map = {
    'Timestamp': 'timestamp', 'CustomerID': 'customer_id', 'CustomerName': 'customer_name', 'Phone': 'phone',
    'ApprovedAmount': 'approved_amount', 'OutstandingBalance': 'outstanding_balance', 'MarkedBy': 'marked_by', 'Notes': 'notes'
}

# --- 4. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pull_plug_records ---
pull_plug_map = {
    'Timestamp': 'timestamp', 'CustomerID': 'customer_id', 'CustomerName': 'customer_name', 'Phone': 'phone',
    'PullPlugAmount': 'pull_plug_amount', 'MarkedBy': 'marked_by', 'Notes': 'notes'
}

# --- 5. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö return_principal_records ---
return_principal_map = {
    'Timestamp': 'timestamp', 'CustomerID': 'customer_id', 'CustomerName': 'customer_name', 'Phone': 'phone',
    'ReturnAmount': 'return_amount', 'MarkedBy': 'marked_by', 'Notes': 'notes'
}

# --- 6. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö allpidjob ---
all_pid_jobs_map = {
    'Date': 'transaction_date', 'CompanyName': 'company_name', 'CustomerID': 'customer_id', 'Time': 'transaction_time',
    'CustomerName': 'customer_name', 'interest': 'interest', 'Table1_OpeningBalance': 'table1_opening_balance',
    'Table1_NetOpening': 'table1_net_opening', 'Table1_PrincipalReturned': 'table1_principal_returned',
    'Table1_LostAmount': 'table1_lost_amount', 'Table2_OpeningBalance': 'table2_opening_balance',
    'Table2_NetOpening': 'table2_net_opening', 'Table2_PrincipalReturned': 'table2_principal_returned',
    'Table2_LostAmount': 'table2_lost_amount', 'Table3_OpeningBalance': 'table3_opening_balance',
    'Table3_NetOpening': 'table3_net_opening', 'Table3_PrincipalReturned': 'table3_principal_returned',
    'Table3_LostAmount': 'table3_lost_amount', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô': 'main_assigned_company'
}

# --- 7. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö users ---
users_map = {
    'id': 'id',      # Key ‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô CSV, Value ‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á SQL
    'pass': 'password'
}

def main():
    # --- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ---
    clean_install = '--clean' in sys.argv

    if clean_install:
        print("\nüî•üî•üî• ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà! üî•üî•üî•")
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        confirm = input("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏∞‡∏´‡∏≤‡∏¢‡πÑ‡∏õ! ‡∏û‡∏¥‡∏°‡∏û‡πå 'yes' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ")
        if confirm.lower().strip() != 'yes':
            print("‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
            exit()

    # --- Step 0: Create tables first ---
    # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ clean_install ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
    create_tables(clean_install=clean_install)

    # --- REFACTORED: Define all migration tasks in a structured list ---
    migration_tasks = [
        {
            'csv_path': 'users.csv',
            'table_name': 'users',
            'column_map': users_map,
            'dtype_spec': None
        }
    ]

    # --- REFACTORED: Loop through tasks and execute ---
    for task in migration_tasks:
        csv_path = task['csv_path']
        print(f"\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV: '{csv_path}'")
        try:
            # REVISED: Handle different delimiters for different CSV files.
            # data1.csv and users.csv use commas, while the others use semicolons.
            delimiter = ';' if task['table_name'] not in ['customer_records', 'users'] else ','
            df = pd.read_csv(
                csv_path, 
                encoding='utf-8-sig', 
                dtype=task.get('dtype_spec'), 
                delimiter=delimiter
            )
            process_dataframe_and_import(df, task['table_name'], task['column_map'], f"‡πÑ‡∏ü‡∏•‡πå '{csv_path}'")
        except FileNotFoundError:
            print(f"üö® ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå '{csv_path}'! ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå '{csv_path}': {e}")

    print("\nüéâ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")

if __name__ == '__main__':
    main()